{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNtRl70hd/5pOxGQjtspfXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hxk271/SocDataSci/blob/main/archive/W09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 오늘의 파이썬"
      ],
      "metadata": {
        "id": "_7L7LS6xvwpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 함수의 정의와 패러미터 사용\n",
        "\n",
        "> <b>함수(function)</b>는 프로그램 내에서 특정 작업을 수행하는 코드 블록이다. 이것을 잘 설계하면 반복적인 코드를 줄이고, 코드의 재사용가능성을 높이며, 가독성을 크게 향상시킨다. 우리가 지금껏 사용해온 수많은 함수들도 사실 다른 라이브러리에서 만들어진 것이다. 우리는 그것들을 구태여 의식하지 않고 그 라이브러리를 `import`한 뒤, 거기에 정의된 함수를 사용했다."
      ],
      "metadata": {
        "id": "R3Mbw_2dx3Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greet():         #함수의 정의(definition)\n",
        "    print(\"안녕하세요!\")\n",
        "\n",
        "greet()"
      ],
      "metadata": {
        "id": "jaW4jmXax512"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 앞의 함수 안에는 괄호 안에 아무것도 없다(Why?). 그런데 일반적으로 함수는 (하나 이상의) <b>패러미터(parameter)</b>를 받기 마련이다."
      ],
      "metadata": {
        "id": "aOrZVI-oyaG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name):\n",
        "    print(\"안녕하세요, \" + name + \" 님!\")\n",
        "\n",
        "name = input(\"당신의 이름은 무엇입니까? \")\n",
        "greet(name)"
      ],
      "metadata": {
        "id": "o-9Sdb7VyicZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 또 <b>반환(return)</b>을 잘 사용해야 한다."
      ],
      "metadata": {
        "id": "3vdYja3kyzk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a, b):\n",
        "    summed = a + b\n",
        "    return summed     #반환 값\n",
        "\n",
        "add(10, 4)"
      ],
      "metadata": {
        "id": "LpZ_PrPAy0in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 위 코드를 수행하고 `print(a)`를 확인해보자. 아무런 값도 돌아오지 않는다. 왜냐하면 `a`와 `b`, `summed`같은 변수는 오로지 해당 함수 안에서만 쓰인 변수이기 때문이다!"
      ],
      "metadata": {
        "id": "8iZ8UizezH5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#에러가 없다.\n",
        "def add(a, b):\n",
        "    summed = a + b\n",
        "    print(a)\n",
        "    return summed\n",
        "\n",
        "add(10, 4)\n",
        "\n",
        "#에러를 일으킨다.\n",
        "def add(a, b):\n",
        "    summed = a + b\n",
        "    return summed\n",
        "\n",
        "add(10, 4)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "53HU6sO24IpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 1-1**. 두 정수(integer)를 입력받아 곱을 반환하는 함수를 만드시오."
      ],
      "metadata": {
        "id": "CdW_ANmq0LIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mul(a, b):\n",
        "    return int(a) * int(b)\n",
        "\n",
        "a, b = input(), input()\n",
        "print(a + \"x\" + b + \"=\" + str(mul(a, b)))"
      ],
      "metadata": {
        "id": "cVkkEE780SQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 1-2**. 두 실수를 입력받아, 그 중 보다 큰 값을 골라주는 함수를 작성하시오."
      ],
      "metadata": {
        "id": "u1Vq5gng4rjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lt(number1, number2):\n",
        "    return max(int(number1), int(number2))        # max(a,b) 함수를 공부하자\n",
        "\n",
        "a, b = input(), input()\n",
        "print(\"결과:\", lt(a, b))"
      ],
      "metadata": {
        "id": "MzHbxV0I4rjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 지금까지 파이썬에서 외부 라이브러리 함수를 굉장히 많이 썼지만 은근슬쩍 이야기하지 않았던 부분이 있다. 그것은 패러미터의 입력 순서에 관한 것이다! 먼저 아래 두 코드에서 차이가 무엇인지 살펴보자."
      ],
      "metadata": {
        "id": "j9b8SHWpLcBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#패러미터 입력 순서 1\n",
        "def sub(a, b):\n",
        "    subtracted = a - b\n",
        "    return subtracted     #반환 값\n",
        "print(sub(10, 4))\n",
        "\n",
        "#패러미터 입력 순서 2\n",
        "def sub(a, b):\n",
        "    subtracted = a - b\n",
        "    return subtracted     #반환 값\n",
        "print(sub(a=10, b=4))\n",
        "\n",
        "#패러미터 입력 순서 3\n",
        "def sub(a, b):\n",
        "    subtracted = a - b\n",
        "    return subtracted     #반환 값\n",
        "print(sub(4, 10))\n",
        "\n",
        "#패러미터 입력 순서 4\n",
        "def sub(a, b):\n",
        "    subtracted = a - b\n",
        "    return subtracted     #반환 값\n",
        "print(sub(b=4, a=10))"
      ],
      "metadata": {
        "id": "zYvWFxiuLqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 즉 함수에서 구체적으로 어떤 패러미터(위 코드에서 `a` 또는 `b`)에 특정 값을 집어넣을지 명확히 한다면 순서는 지키지 않아도 된다. 그러나 그렇지 않다면 순서를 명확히 지켜야 한다!"
      ],
      "metadata": {
        "id": "hPCd5I-1MHMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 09 (자료의 전처리 I)\n",
        "\n",
        "실무나 연구를 수행할 때 여러분에게 주어지는 자료는 종종 예쁘게 정돈되어(tidy) 있지 않고 엉망진창인 상태로 들어온다. 이러한 자료를 분석에 투입할 수 있게 가다듬는 과정을 <b>전처리(pre-processing)</b>라고 부른다. 전처리는 보통 데이터 분석에서 가장 긴 시간을 소모한다. 보통 재미가 없지만(놀랍지만 취향을 탄다!) 반드시 필요하므로 산업계/연구계에서 인력 수요가 높다. 게다가 (적어도 현시점에서) AI에 의한 대체가능성이 그리 높지 않다(Why?).\n",
        "\n",
        "다음의 기술들이 기본적으로 필요하다.\n",
        "\n",
        "(1) **재부호화(recoding)** 및 새로운 변수 계산\\\n",
        "(2) 자료의 **조건부 필터링(conditional filtering)**\\\n",
        "(3) **결측치(missing values)** 처리\\\n",
        "(4) 자료 **결합(merging)** \\\n",
        "(5) 자료의 **재배열(reshaping)**\n",
        "\n",
        "전처리를 익숙하게 하려면 수많은 경험을 통해 스스로 노하우를 축적해야 한다(우리가 하려는 것은 그야말로 전처리의 찍먹 수준이다)! 이번 주와 다음 주에 걸쳐 자료의 전처리(data preprocessing)를 연습하게 된다. 이를 위해 무엇보다 pandas을 좀 더 다재다능하게 활용할 수 있어야 한다."
      ],
      "metadata": {
        "id": "ukPD8G6gnXOZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWn55b_1McNM"
      },
      "source": [
        "##2. 수열에서 데이터프레임으로\n",
        "\n",
        "> 수학적으로 보면 자료(data)란 결국 행렬(matrix)이고, 하나의 변수란 <b>벡터(vector)</b>**인 셈이다(Why?). pandas에서도 이 점을 반영하여 하나의 변수를 수열(series)로 구현하고 있다(책에 따라선 이걸 배열이라고 번역한다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "d9842731"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [3.7, 3.8, 3.8, 3.9, 3.8, 4.0, 3.6, 2.9, 2.7, 2.8]\n",
        "unemployment = pd.Series(data)\n",
        "unemployment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 아래 코드를 한줄씩 입력하여 pandas 수열의 타입과 내용을 차분히 확인해보자."
      ],
      "metadata": {
        "id": "M8iRO6rBOUpr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1b2fa6d"
      },
      "outputs": [],
      "source": [
        "type(unemployment)        #Series다!\n",
        "\n",
        "unemployment.index        #수열의 한줄한줄에 붙은 인덱스를 확인한다.\n",
        "unemployment.values       #수열의 내용, 즉 값(values)을 본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fba30db"
      },
      "source": [
        "> 인덱스가 있으면 구체적인 <code>pd.Series</code>의 일부를 불러올 때 편리하다! 수열에 대해 별도의 인덱스를 줄 수 있지만 구태여 그렇게까지는 잘 하지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e561183c"
      },
      "outputs": [],
      "source": [
        "unemployment.iloc[4]         #4번째 관찰값"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 하나의 변수는 하나의 수열, 즉 <code>pd.Series</code>이고, 하나의 데이터는 곧 하나의 행렬, 즉 <code>pd.DataFrame</code>이 된다. 수열을 어떻게 하면 데이터프레임으로 전환할 수 있을까? pandas 데이터프레임은 JSON 포멧을 따르고 있다. JSON 포멧은 별도의 공부가 필요하지만 일단 dict의 확장판이라고 생각하자."
      ],
      "metadata": {
        "id": "swKS1ryKdsy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inflation = pd.Series([2.2, 1.6, 1.5, 1.2, 0.9, 0.7, 1.8, 4.1, 4.0, 2.1])\n",
        "\n",
        "#이때 data가 JSON 포멧을 따르고 있음에 주의하자\n",
        "data = {\"inflation\": inflation,\n",
        "        \"unemployment\": unemployment}\n",
        "\n",
        "#json 포멧을 괄호 안에 집어넣으면 df가 된다.\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "vk7KYa5IdtOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 2-1**. 실업률(`unemployment`)과 인플레이션율(`inflation`)으로 만든 데이터에 연도(`year`) 변수를 추가하자. 이때 연도는 2015년에서 2024년까지이다."
      ],
      "metadata": {
        "id": "gKbuKC3sjZZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = pd.Series(range(2015, 2025))\n",
        "\n",
        "data = {\"year\": year,\n",
        "        \"inflation\": inflation,\n",
        "        \"unemployment\": unemployment}\n",
        "data\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "tnWxlBGqaI7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그런데 위의 방식은 한줄한줄의 변수(수열)을 데이터프레임(행렬)으로 변환한 것이다. 그와는 달리 한줄한줄의 관측치(이것도 수열이라면 수열)를 데이터프레임(행렬)으로 변환할 수도 있다. 가령 아래의 관측치와 `myjson`을 살펴보자. 이것이 위의 방식과 어떻게 다른지 고민해보자!"
      ],
      "metadata": {
        "id": "TCCvJnLyul2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec1 = {\"id\": 1001,\n",
        "      \"name\": \"김전일\",\n",
        "      \"motto\" : \"할아버지의 이름을 걸고\"}\n",
        "\n",
        "rec2 = {\"id\": 1002,\n",
        "      \"name\": \"코난\",\n",
        "      \"motto\" : \"진실은 언제나 하나\"}\n",
        "\n",
        "rec3 = {\"id\" : 1003,\n",
        "      \"name\" : \"김현우\",\n",
        "      \"motto\" : \"할많하않\"}\n",
        "\n",
        "myjson = {'data': [rec1, rec2, rec3]}\n",
        "myjson\n",
        "myjson['data']"
      ],
      "metadata": {
        "id": "3EdV1SbcumQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이제 괄호 안에 리스트 포멧(JSON 포멧이 아니다!)인 `myjson['data']`를 넣으면 데이터프레임이 된다. 만일 괄호 안에 `myjson`을 넣으면 어떻게 되는지 살펴보자."
      ],
      "metadata": {
        "id": "UR7G5HUCK2fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#왜 이상할까?\n",
        "df = pd.DataFrame(myjson)\n",
        "df\n",
        "\n",
        "#이거다!\n",
        "df = pd.DataFrame(myjson['data'])\n",
        "df"
      ],
      "metadata": {
        "id": "D6zsf8oUK1dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0be6a8"
      },
      "source": [
        "> 만일 변수의 이름을 바꾸고 싶다면 어떻게 할까?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff8b4f39"
      },
      "outputs": [],
      "source": [
        "df.columns\n",
        "df.columns = ['아이디', '이름', '좌우명']\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그러나 특정 변수의 이름만을 바꿀 때는 `rename()` 함수를 사용한다. 크게 두 가지 방법이 있는데 원하는 방식을 사용하면 된다. 이때 `axis` 옵션을 사용한다면 `axis=0`을 통해 인덱스(관측치의 이름)를 바꿀 수 있고, `axis=1`을 통해 컬럼(변수의 이름)을 바꿀 수 있음에 주의하자."
      ],
      "metadata": {
        "id": "0dXYHbiCwnj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.rename({'아이디': 'id',\n",
        "                 '이름': 'name',\n",
        "                 '좌우명': 'motto'},\n",
        "                axis=1)\n",
        "df2"
      ],
      "metadata": {
        "id": "ChGsx94ngsof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이때 `rename()`도 사실 pandas의 매서드(method)일 뿐이다. 그리고 `axis` 뿐 아니라, 그 앞의 딕트도 사실 패러미터로 투입된 것이다. 그런데 원래는 어떤 패러미터의 이름을 가졌을까? 간단히 살펴볼 수 있다."
      ],
      "metadata": {
        "id": "l6fnylFaMu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "? df.rename"
      ],
      "metadata": {
        "id": "sxbygPGoM4vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 사실은 `mapper`라는 패러미터의 진짜 이름이 있다. 그리고 `axis`도 네번째 패러미터였다."
      ],
      "metadata": {
        "id": "jqheryerNcMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#따라서 이렇게 할 수 있다.\n",
        "df2 = df.rename(mapper = {'아이디': 'id',\n",
        "                          '이름': 'name',\n",
        "                          '좌우명': 'motto'},\n",
        "                axis=1)\n",
        "df2\n",
        "\n",
        "#그러나 이렇게 할 수는 없다(Why?).\n",
        "df2 = df.rename({'아이디': 'id',\n",
        "                 '이름': 'name',\n",
        "                 '좌우명': 'motto'},\n",
        "                1)"
      ],
      "metadata": {
        "id": "sr6KRpITNikx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 2-2**. `axis=0`이면 어떻게 되는지 연습해보자.\n",
        ">\n",
        ">---\n",
        ">```python\n",
        ">df2 = df.rename({2: 'OMG'},\n",
        ">                axis=0)\n",
        ">df2\n",
        ">```"
      ],
      "metadata": {
        "id": "7Nh0wNsGSAyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 만일 특정 변수, 가령 `아이디` 변수를 제거하려면 어떻게 할까? 크게 두 가지 방법이 널리 쓰인다!"
      ],
      "metadata": {
        "id": "dwUrf0smR4ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#id만 빼고 추출한다.\n",
        "df2 = df[['이름', '좌우명']]\n",
        "df2\n",
        "\n",
        "#drop() 매서드\n",
        "df3 = df.drop('아이디', axis=1)\n",
        "df3"
      ],
      "metadata": {
        "id": "6rz1pDBASCER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 2-3**. 아래 남녀 실업률 자료(2010-2024)를 데이터프레임으로 전환하시오.\n",
        "---\n",
        "```python\n",
        "male = [4.1, 3.7, 3.4, 3.4, 3.6, 3.7, 3.9, 3.9, 4.0, 4.0, 4.0, 3.6, 2.7, 2.7, 2.7]\n",
        "female = [3.4, 3.2, 3.1, 3.0, 3.6, 3.7, 3.7, 3.6, 3.8, 3.6, 4.0, 3.7, 3.0, 2.7, 2.8]\n",
        "```"
      ],
      "metadata": {
        "id": "yye1P9_ROWJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male = [4.1, 3.7, 3.4, 3.4, 3.6, 3.7, 3.9, 3.9, 4.0, 4.0, 4.0, 3.6, 2.7, 2.7, 2.7]\n",
        "female = [3.4, 3.2, 3.1, 3.0, 3.6, 3.7, 3.7, 3.6, 3.8, 3.6, 4.0, 3.7, 3.0, 2.7, 2.8]\n",
        "\n",
        "year = range(2010, 2025)\n",
        "data = {\"year\": year,\n",
        "        \"unemp_male\": male,\n",
        "        \"unemp_female\": female}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df[['unemp_male', 'unemp_female']].plot()"
      ],
      "metadata": {
        "id": "sA3oJrgHOWJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 행과 열의 필터링\n",
        "\n",
        "> 좀 더 본격적으로 데이터프레임을 다루는 방식을 배우자. 자전거 대여 데이터를 사용하여 행과 열을 선택하는 방법을 본격적으로 연습해보자.\n",
        ">\n",
        "> ---\n",
        "> ```python\n",
        "> import gdown\n",
        "> link = 'https://drive.google.com/uc?id=1cThPIaLFmJRUm8mRCXRTTxhH5aNj9t_H'\n",
        "> gdown.download(link)\n",
        "> ```"
      ],
      "metadata": {
        "id": "C-Nx50bEfslI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('bike_rentals.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "BK-XopsieDLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `df.iloc`을 다시 연습해보자. (전에도 배웠지만) 이 함수는 정말 능수능란하게 사용할 수 있어야 한다!"
      ],
      "metadata": {
        "id": "buURZ95IfSeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#iloc을 이용한 행 선택\n",
        "df.iloc[2:5]\n",
        "\n",
        "#iloc을 이용한 행과 열 선택\n",
        "df.iloc[2:5, 3:6]"
      ],
      "metadata": {
        "id": "7Td5VUT1fIle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy7bIAdYmnGf"
      },
      "source": [
        "> `iloc[,]`에서 쉼표 앞과 뒤를 구분하자. 앞은 행(rows)을, 뒤는 열(columns)을 지시한다. 가령 `iloc[1:3, 2:4]`는 1행에서 2행까지, 그 안에서 다시 2열에서 3열까지 자료만 따로 슬라이싱하게 된다. 물론 `iloc[:, :]`은 데이터 전체이다(Why?)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0번째 index 1번째 column\n",
        "df.iloc[0,1]\n",
        "\n",
        "#0번째 index 1번째 column부터, 5번째 index 3번째 column까지 부분행렬\n",
        "df.iloc[0:5, 1:3]\n",
        "\n",
        "#1번째 column부터, 3번째 column까지 부분행렬\n",
        "df.iloc[:, 1:3]"
      ],
      "metadata": {
        "id": "OgCK2tsXL3op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `iloc` 말고 `loc`도 쓸모가 있다. 이것은 특히 여러분이 직접 인덱스를 부여했을 때 중요하다(이것에 관해서는 곧 배운다). `iloc`은 <b>integer-location</b>이므로 오로지 숫자만 받지만, `loc`에서는 문자열도 사용할 수 있게 된다."
      ],
      "metadata": {
        "id": "Fs3BMc3BOEik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#0에서 9까지(10 미포함)\n",
        "df.iloc[0:10]\n",
        "\n",
        "#0에서 10까지(10 포함)\n",
        "df.loc[0:10]\n",
        "\n",
        "#loc에서는 변수명을 직접 줄 수 있다\n",
        "df.loc[df['season'] == 2]                  #df[df['season'] == 2] 와 똑같다!\n",
        "\n",
        "#season이 2인 행들 중에 casual, resistered, count 열만 선택\n",
        "df.loc[df['season'] == 2, 'casual':]\n",
        "\n",
        "#season이 1이 아니면서 동시에 weather가 2가 아닌 모든 행 선택\n",
        "df.loc[(df['season'] != 1) & (df['weather'] != 2)]"
      ],
      "metadata": {
        "id": "H2UOkHVOOM1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 3-1**. 위 자료에서 15번-21번 관측치의 `workingday`부터 `temp`까지의 변수를 담은 부분행렬을 출력하시오."
      ],
      "metadata": {
        "id": "GH_zTzQC0UKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[15:21, 'workingday':'temp']"
      ],
      "metadata": {
        "id": "hfwCukUZ0yyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 3-2**. `season`이 1이 아니면서 동시에 `weather`가 2가 아닌 모든 행을 선택하시오."
      ],
      "metadata": {
        "id": "MCtX5nmYtto-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[~(df['season'] == 1) & ~(df['weather'] == 2)]\n",
        "#또는\n",
        "df.loc[(df['season'] != 1) & (df['weather'] != 2)]"
      ],
      "metadata": {
        "id": "OE6YR2lPumy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 몇 가지 중요한 정보, 특히 각 변수의 속성(dtype)과 유효 사례수를 알 수 있었다. 그런데 `datetime`의 경우에는 날짜와 시간 속성이지 사실 object는 아니다. 이를 변환해보자."
      ],
      "metadata": {
        "id": "vxLxHyl0gUYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#지난 주에도 배웠다.\n",
        "df.info()\n",
        "\n",
        "#Dtype을 object에서 datetime으로!\n",
        "df['date'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "#제대로 바뀌었나 확인해보자\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ABMUqrnDCbzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그런데 이 데이터를 잘 살펴보자! 이 자료는 사실 <b>시계열 자료(time-series data)</b>이므로 날짜/시간을 인덱스(index)로 설정할 수도 있다(Why?). 이렇게 인덱스를 설정하면 `date` 변수는 사라지고 인덱스 취급을 받게 된다."
      ],
      "metadata": {
        "id": "CUdllVz9C1Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.set_index('date')\n",
        "df"
      ],
      "metadata": {
        "id": "mqD1s11mgT75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이제 `df.iloc`을 `df.loc`을 다르게 사용할 수 있다."
      ],
      "metadata": {
        "id": "ZCBtxSsIwawn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]\n",
        "df.loc['2011-01-01 00:00:00']\n",
        "\n",
        "#시간을 인덱스로 설정하면 언제부터 언제까지 검색할 때 편리하다!\n",
        "df.loc['2012-12-18 19:00:00':'2012-12-19 19:00:00']"
      ],
      "metadata": {
        "id": "xfwwb-1OgjtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 인덱스 설정을 되돌리고 싶다면 `reset_index()`를 사용한다. 단 `df = df.reset_index()`를 사용해야 리셋된 내용이 `df`로 대입된다는 점을 잊지 말자."
      ],
      "metadata": {
        "id": "XteOpePs6-3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index()"
      ],
      "metadata": {
        "id": "LOQmjVyW7CWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 인덱스 설정은 꼭 필요한 것은 아니다. 그러나 후속 작업을 편리하게 해주기 때문에 적절하게 사용한다면 인생이 편해진다.\n",
        ">\n",
        "> 가령 위 데이터의 경우 `resample()` 같은 매서드를 사용하면 매일매일의 평균이나 합계 등을 금방 계산할 수도 있다."
      ],
      "metadata": {
        "id": "5gB2okf9Ezsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.resample('24h').mean(numeric_only=True)"
      ],
      "metadata": {
        "id": "SogoywWIEyRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 3-3**. 아래 자료를 사용하여 날짜를 인덱스로 설정하시오.\n",
        ">\n",
        ">---\n",
        ">```python\n",
        ">import gdown\n",
        ">link = 'https://drive.google.com/uc?id=1cb5Nxgk-hGfoTNsy8VajAqb4oMlJKfff'\n",
        ">gdown.download(link)\n",
        ">```"
      ],
      "metadata": {
        "id": "qaRg4T7JF9aJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7cbdbd1"
      },
      "source": [
        "## 4. 인덱스 설정과 재설정\n",
        "\n",
        "> 데이터프레임을 불러올 때 (인덱스가 있으면) 설정해 두는 것이 결국 나중에 편리하다고 앞서 설명했다. 인덱스가 꼭 숫자여야 할 필요는 없다. 하지만 (그 목적을 생각할 때) 고유한(unique) 것이 바람직하다!\n",
        ">\n",
        "> 이번엔 다른 데이터셋을 사용하여 인덱스 관리를 좀 더 철저히 연습해보자."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "link = 'https://drive.google.com/uc?id=1bXEgep5eQ__R2kOIHD5x2hP57IOAtktV'\n",
        "gdown.download(link)"
      ],
      "metadata": {
        "id": "4b3lrzwhh1OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 데이터를 불러올 때부터 아예 인덱스를 설정할 수 있다."
      ],
      "metadata": {
        "id": "Z4_SeaHyfLml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "b30f0e00"
      },
      "outputs": [],
      "source": [
        "alco2009 = pd.read_csv(\"niaaa-report2009.csv\")\n",
        "alco2009\n",
        "\n",
        "#index_col을 사용한다\n",
        "alco2009 = pd.read_csv(\"niaaa-report2009.csv\", index_col = \"State\")\n",
        "alco2009"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이제 0번째 관측치인 Alabama를 두 가지 방식으로 찾아보자."
      ],
      "metadata": {
        "id": "RXj2hxZiohOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#iloc\n",
        "alco2009.iloc[0]\n",
        "\n",
        "#loc\n",
        "alco2009.loc[\"Alabama\"]"
      ],
      "metadata": {
        "id": "rlmXOhKcohoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3ae282"
      },
      "source": [
        "> (일단 인덱스가 설정되어 있으면) 특정 인덱스가 데이터 안에 있나 없나도 금방 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b86eef65"
      },
      "outputs": [],
      "source": [
        "\"Alabama\" in alco2009.index\n",
        "\"Samoa\" in alco2009.index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그냥 데이터를 불러왔더라도 나중에 <code>set_index()</code>를 사용하면, 기존의 변수를 새로운 인덱스로 설정할 수 있다."
      ],
      "metadata": {
        "id": "F52hIqJ1n_OQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "54dcac37"
      },
      "outputs": [],
      "source": [
        "alco2009 = pd.read_csv(\"niaaa-report2009.csv\")\n",
        "alco2009\n",
        "\n",
        "#왜 요상한지 생각해보자.\n",
        "alco2009 = alco2009.set_index(\"Beer\")\n",
        "alco2009"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <code>reset_index()</code>을 통해 기존 인덱스를 그냥 변수로 내보내고 새로운 인덱스를 만들어 줄 수도 있다."
      ],
      "metadata": {
        "id": "t7K3113Ooc9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alco2009 = alco2009.reset_index()\n",
        "alco2009"
      ],
      "metadata": {
        "id": "Iyg1mLcCny8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfc63817"
      },
      "source": [
        "> 심지어 두 개 이상의 변수를 사용하여 인덱스를 설정할 수도 있다. 이것을 <b>멀티인덱스(MultiIndex)</b>라고 부르고, 그룹 단위로 자료를 <b>집계(aggregation)</b>를 할 때 종종 유용하게 사용된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "813a3dc1"
      },
      "outputs": [],
      "source": [
        "link = 'https://drive.google.com/uc?id=1bYQAy0y4zMGxWRZ4jPvv2BLkMIunuuG-'\n",
        "gdown.download(link)\n",
        "\n",
        "#국가-연도를 관찰단위로 하는 경우도 제법 흔하다!\n",
        "alco = pd.read_csv(\"niaaa-report.csv\", index_col = [\"State\", \"Year\"])\n",
        "alco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEAuZ3h5McNP"
      },
      "source": [
        "> **연습문제 4**. `niaaa-report2009.csv` 파일을 인덱스없이 불러오시오. Pennsylvania 주의 정보를 여러 가지 방법으로 확인해보시오. 그 다음, 주 이름(`States`)으로 인덱스를 설정하시오. 다시 한 번 Pennsylvania 주의 정보를 여러 가지 방법으로 확인해보시오."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#인덱스 없이 찾아보기\n",
        "alco2009 = pd.read_csv(\"niaaa-report2009.csv\")\n",
        "alco2009.iloc[38]                                 #iloc()\n",
        "alco2009[alco2009['State'] == 'Pennsylvania']     #마스킹\n",
        "\n",
        "#인덱스 설정하여 찾아보기\n",
        "alco2009 = alco2009.set_index(\"State\")\n",
        "alco2009.loc['Pennsylvania']                      #loc()"
      ],
      "metadata": {
        "id": "MsIKKNcPn-YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 빈도분포표\n",
        "\n",
        "> 사회통계학 수업에서 <b>빈도분포표(frequency distribution table)</b>에 관해 이미 배웠다. 굉장히 유용하고 자주 사용된다.\n",
        ">\n",
        "> 이 연습을 위해 먼저 유명 호텔 예약 사이트인 booking.com의 일부 호텔에 대한 이름, 평점, 위치에 관한 데이터를 불러오자."
      ],
      "metadata": {
        "id": "9W3u6dyZkAp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "link = 'https://drive.google.com/uc?id=1cVJTA4MnZ62eC2jQ_V_iN92zG8A6aLxc'\n",
        "gdown.download(link)"
      ],
      "metadata": {
        "id": "RFMJlnzciH3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Bookings.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "4XCu-TqAg9vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `info()`와 `describe()`는 각각 쓸모가 다르다."
      ],
      "metadata": {
        "id": "F69dGSn81LRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#info 메서드를 이용한 데이터테이블의 개략적 정보 파악\n",
        "df.info()\n",
        "\n",
        "#describe 메서드를 이용한 데이터테이블의 수치형 통계정보 파악\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "YybjNfWvhPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 빈도분포표를 출력해보자. 빈도분포표는 하나의 변수에 대해 얻어지는 것이다. 빈도분포표가 `df.describe()`와 다르다는 것을 이해하자."
      ],
      "metadata": {
        "id": "450d7U6Nhg5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 고유값과 고유값들이 나타나는 빈도수 구하기\n",
        "df['Review'].value_counts()"
      ],
      "metadata": {
        "id": "Upq6wGC5hgOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그런데 빈도분포표를 잘 살펴보면 `Review score`와 `Superb 9.0`, `Exceptional 10`가 좀 이상하다는 걸 알 수 있다(Why?). 전처리가 되지 않은 자료에서 이런 문제는 제법 흔하며, 분석 전에 세심하게 살펴보고 모두 수정해야 한다. 검색과 마찬가지 요령으로 `df.loc()`을 사용하고 그 뒤에 새로운 값을 직접 대입할 수 있다."
      ],
      "metadata": {
        "id": "_vEkWMA9h52A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond1 = (df['Review'] == \"Superb 9.0\")\n",
        "df.loc[cond1, \"Review\"] = \"Superb \"      #원본 데이터에 빈칸이 뒤에 들어있었다.\n",
        "df['Review'].value_counts()"
      ],
      "metadata": {
        "id": "pICMOHjXhmgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 5-1**. `Exceptional 10`도 올바르게 수정해보자."
      ],
      "metadata": {
        "id": "ARAW1wxXizmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond2 = df['Review'] == \"Exceptional 10\"\n",
        "df.loc[cond2, \"Review\"] = \"Exceptional \"      #원본 데이터에 빈칸이 뒤에 들어있었다.\n",
        "df['Review'].value_counts()"
      ],
      "metadata": {
        "id": "6KgwfZ-w2ATY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 빈도분포표와는 무관하게 고유한 <b>응답 문항(response items)</b>만을 살펴볼 수도 있다."
      ],
      "metadata": {
        "id": "egrmOkzd1nkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Total_Review'].unique()"
      ],
      "metadata": {
        "id": "jsYYYgoqi6Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 5-2**. 호텔이 위치한 지역의 빈도분포표와 고유 응답문항 목록을 살펴보시오."
      ],
      "metadata": {
        "id": "22bMMZIS11PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#두 매서드의 차이가 무엇인지 생각해보자.\n",
        "df['Location'].value_counts()\n",
        "df['Location'].unique()"
      ],
      "metadata": {
        "id": "gQxXbABA1wie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 결측치 처리\n",
        "\n",
        "> <b>결측치(missing values)</b>는 모든 자료에서 거의 필연적으로 발생한다. 결측치가 없는 대규모 자료는 대체로 다음 둘 중 하나 이상의 원인에 의해 발생한다: 조작 또는 연구윤리 위반.\n",
        ">\n",
        "> 먼저 결측치를 발견하는 방법부터 알아야 한다. 이때는 `df.info()`를 통해 결측치가 어떤지 우선 살펴보자. 그리고 나서 이를 어떻게 대체하면 좋은지 알아야 하는데, 대표적인 방법으로 특히 `isna()`, `notna()`, `fillna()`, 그리고 `dropna()` 등이 유용하다."
      ],
      "metadata": {
        "id": "-0q5Q3DBjoXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "kv_yzEkn74av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 결측치가 가장 많은 것은 `Rating`이므로 이것을 잘 살펴보자. `isna()`를 활용한 마스킹(masking)을 통해 결측치가 담겨있는 관측치만을 살펴볼 수 있다. 결측치 판정은 당연히 변수를 기준으로 한다."
      ],
      "metadata": {
        "id": "v_oWwkEz8Au_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond = df['Rating'].isna()\n",
        "df[cond]\n",
        "#또는\n",
        "cond = df['Rating'].isnull()\n",
        "df[cond]"
      ],
      "metadata": {
        "id": "srj9VCXy8H1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 반대로 결측치가 아닌 멀쩡한 관측치는 `notna()` 또는 `notnull()`을 사용한다!"
      ],
      "metadata": {
        "id": "NiiRdMo-2zrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Rating'].notna()]\n",
        "#또는\n",
        "df[df['Rating'].notnull()]"
      ],
      "metadata": {
        "id": "zj8HHblO21EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 구체적으로 어떤 관측치에 결측치가 있는지 그 인덱스 번호만 확인하려면 어떻게 할까? 아래의 코드를 한줄씩 살펴보자."
      ],
      "metadata": {
        "id": "WD-WLT-18Vpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond = df['Rating'].isna()\n",
        "missingindex = df[cond].index      # index 속성(attribute)을 활용하면 인덱스 목록이 반환되었을 떠올리자.\n",
        "missingindex\n",
        "df.iloc[missingindex]"
      ],
      "metadata": {
        "id": "PWANzedqkWBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 결측치가 담긴 관측치를 아예 제거할 수 있다. 만일 `df.dropna()`를 사용하면 어떤 변수에라도 결측치가 있는 관측치(=인덱스)는 모조리 삭제된다. 이 데이터에서 본래 관측치가 525개였으나 이렇게 지우고 나오면 307개만 남는다."
      ],
      "metadata": {
        "id": "mFX8F3nb96Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df=df.dropna()를 실행하면 결측치가 없는 데이터만 남게 된다.\n",
        "df.dropna()"
      ],
      "metadata": {
        "id": "7frGmcN89tma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 만약에 특정 변수에 결측치가 있는 경우에만 삭제하고 싶다면 패러미터로 `subset`을 활용하자."
      ],
      "metadata": {
        "id": "HGWgTitb96Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset='Total_Review')"
      ],
      "metadata": {
        "id": "3I_4ORUc-MlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 결측치를 함부로 제거하는 것이 늘 바람직한 것은 아니다(Why?). 만약 지우기보다 어떻게든 메꾸기를 원한다면 어떻게 하면 좋을까? 우선 <b>평균대체(mean imputation)</b>를 활용할 수 있다."
      ],
      "metadata": {
        "id": "SjaSQviz9TEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#변수의 평균 구하기\n",
        "df['Rating'].mean()\n",
        "\n",
        "# 해당 열의 평균값으로 결측치를 대체\n",
        "tbf = df['Rating'].mean()\n",
        "df['Rating_filled'] = df['Rating'].fillna(tbf)\n",
        "df"
      ],
      "metadata": {
        "id": "owqrI0Jp9Z5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `df.loc`을 활용해 `Rating`에 결측치가 있던 관측치만을 선택해 출력해보자. 제대로 `NaN`이 평균값인 `7.883492`으로 대체되었음을 확인해보자."
      ],
      "metadata": {
        "id": "TkIupu_w9vQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[missingindex, ['Rating', 'Rating_filled']]"
      ],
      "metadata": {
        "id": "r2qTF13C8ipX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 6-1**. `Rating` 컬럼의 결측치를 중앙값으로 대체하시오."
      ],
      "metadata": {
        "id": "JIaNDbp2_MZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Rating_filled2'] = df['Rating'].fillna(df['Rating'].median())\n",
        "df"
      ],
      "metadata": {
        "id": "_PJM-1M-_GHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 평균대체가 그렇게 훌륭한 방법일까? 한 번 간단히 실험해보자. 먼저 numpy라는 라이브러리를 불러오자. pandas는 엑셀이나 SPSS처럼 데이터 분석에 편리한 도구라면, numpy는 수학 및 공학을 위한 계산(computation) 도구에 가깝다."
      ],
      "metadata": {
        "id": "JNjkduEK12L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NOFmnXth2sZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 아까 만들었던 인플레이션 자료의 5번째 관측치를 일부러 결측치로 만들어보자."
      ],
      "metadata": {
        "id": "JthSpKDo2twS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "f197e890"
      },
      "outputs": [],
      "source": [
        "#멀쩡한 자료\n",
        "inflation = pd.Series([2.2, 1.6, 1.5, 1.2, 0.9, 0.7, 1.8, 4.1, 4.0, 2.1])\n",
        "inflation\n",
        "\n",
        "#일부러 4번째 줄에 구멍내기\n",
        "inflation.iloc[4] = np.nan\n",
        "\n",
        "#어떻게 변했을까?\n",
        "inflation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3287606"
      },
      "source": [
        "> 우리는 사실 일부러 결측치를 발생시켰기 때문에 진정한 답이 무엇인지 알고 있다(0.9). 평균대체를 하면 어떻게 될까?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b2a769f"
      },
      "outputs": [],
      "source": [
        "inflation = pd.Series([2.2, 1.6, 1.5, 1.2, 0.9, 0.7, 1.8, 4.1, 4.0, 2.1])\n",
        "inflation.iloc[4] = np.nan\n",
        "inflation\n",
        "\n",
        "#mean with a missing value\n",
        "tbi = inflation.mean()\n",
        "tbi\n",
        "\n",
        "inflation_imputed = inflation.fillna(tbi)\n",
        "inflation_imputed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 우리가 일부러 구멍낸 4번째 관측치의 참된 값은 0.9였다. 그것이 2.13으로 치솟았다. 평균대체는 이런 문제가 있다. 그러면 <b>중앙값 대체(median imputation)</b>이 더 우수할까?"
      ],
      "metadata": {
        "id": "U3Uhdd5p5Khd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inflation = pd.Series([2.2, 1.6, 1.5, 1.2, 0.9, 0.7, 1.8, 4.1, 4.0, 2.1])\n",
        "inflation.iloc[4] = np.nan\n",
        "inflation\n",
        "\n",
        "#median with a missing value\n",
        "tbi = inflation.median()\n",
        "tbi\n",
        "\n",
        "inflation_imputed = inflation.fillna(tbi)\n",
        "inflation_imputed"
      ],
      "metadata": {
        "id": "mHHQM7En5KsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1.8로 조금 더 참된 값에 근접하였으나 여전히 크게 나아지지는 못했다.\n",
        ">\n",
        "> 평균대체는 사실 보다 유사한 속성(또는 거리가 가까운 속성)을 지닌 자료들 \"안\"에서 적용되어야 더 우수한 성과를 보인다. 우리 자료가 <b>시계열 자료</b>(time-series data)라는 것을 감안하면 <b>보간법(interpolation)</b>이 괜찮은 대안이 될 수 있다. 왜냐하면 (4번째 결측치와 거리가 가까운) 3번째 유효값과 5번째 유효값의 평균대체가 더 잘 작동하기 때문이다."
      ],
      "metadata": {
        "id": "rG_s5AIC6CnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inflation = pd.Series([2.2, 1.6, 1.5, 1.2, 0.9, 0.7, 1.8, 4.1, 4.0, 2.1])\n",
        "inflation.iloc[4] = np.nan\n",
        "inflation\n",
        "\n",
        "#median with a missing value\n",
        "tbi = (inflation[3] + inflation[5]) /2\n",
        "tbi\n",
        "\n",
        "inflation_imputed = inflation.fillna(tbi)\n",
        "inflation_imputed"
      ],
      "metadata": {
        "id": "Uzg_gjLL6K0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이제 0.95로 참된 값에 몹시 근접하게 됨을 알 수 있다. 이러한 보간법을 쉽게 수행하기 위해 아예 `interpolate()`라는 함수가 따로 있다."
      ],
      "metadata": {
        "id": "84RK46-D7eUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inflation = pd.Series([2.2, 1.6, 1.5, 1.2, 0.9, 0.7, 1.8, 4.1, 4.0, 2.1])\n",
        "inflation.iloc[4] = np.nan\n",
        "inflation.interpolate()"
      ],
      "metadata": {
        "id": "c7SBncnB6jpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **연습문제 6-1**. 다음 `lifeexp.csv` 자료에서 `safewater`가 결측치인 국가들의 `popgrowth` 평균값과 그렇지 않은 국가들의 `popgrowth` 평균값을 각각 구하시오.\n",
        ">\n",
        ">---\n",
        ">```python\n",
        ">import gdown\n",
        ">link = 'https://drive.google.com/uc?id=1WLQ60NVo0wtrtCRf1odO-VzLmZ7ebVao'\n",
        ">gdown.download(link)\n",
        ">```"
      ],
      "metadata": {
        "id": "ImhzOGlO3Ntf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('lifeexp.csv', index_col = 'country')\n",
        "df\n",
        "\n",
        "#결측치 국가군\n",
        "cond1 = df['safewater'].isnull()\n",
        "print(df[cond1]['popgrowth'].mean())\n",
        "\n",
        "#비-결측치 국가군\n",
        "cond2 = df['safewater'].notnull()\n",
        "print(df[cond2]['popgrowth'].mean())"
      ],
      "metadata": {
        "id": "3i4pqght3RI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 중복 제거\n",
        "\n",
        "> 자료가 중복되어있다면 어떨까? 이걸 제거하는 것도 중요한 전처리 스킬인데 생각보다 어렵지 않다. 우선 일부러 중복을 포함한 자료를 만들어보자."
      ],
      "metadata": {
        "id": "L3sts9WFNxgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myjson = {'id': [1, 1, 2, 3],\n",
        "          'fname' : ['John', 'John', 'Paul', 'Jack'],\n",
        "          'lname' : ['Xavier-McKinnon', 'Xavier-McKinnon', 'Peters', 'Morgans'],\n",
        "          'age' : [20, 21, 31, 43]}\n",
        "df = pd.DataFrame(myjson)\n",
        "df"
      ],
      "metadata": {
        "id": "VjPns3mtT2YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 왜 이런 중복된 레코드가 생겨났을까? 이런 경우가 제법 흔하기 때문에 주의를 기울여야 한다. 먼저 `duplicated()`는 중복된 레코드의 식별에 사용된다. 아래 코드를 한줄한줄 입력하면서 살펴보자."
      ],
      "metadata": {
        "id": "tMOzJiOPHcZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond1 = df.duplicated(['id', 'fname', 'lname'])\n",
        "cond1\n",
        "df[cond1]"
      ],
      "metadata": {
        "id": "QeVwKp_0ZDW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이때 두번째 레코드만을 중복으로 취급했음에 주의하자. 어느 쪽을 남길 것인가는 분석하는 사람의 판단에 달렸다. 물론 다른 방식으로 할 수도 있다."
      ],
      "metadata": {
        "id": "EoDoWLL3H1Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond2 = df.duplicated(['id', 'fname', 'lname'], keep = False)\n",
        "cond2\n",
        "df[cond2]"
      ],
      "metadata": {
        "id": "kAGqy3dPbBtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 같은 레코드가 두 번 이상 중복되어 입력되었다면 하나만 남기고 나머지는 제거해야 한다. 여러 방법이 있는데, `duplicated()`를 역의 조건(`-`)으로 사용하여 추출할 수 있다(Why?)."
      ],
      "metadata": {
        "id": "EyFUF5ECY8LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond1 = df.duplicated(['id', 'fname', 'lname'])\n",
        "cond1\n",
        "df2 = df[-cond1]\n",
        "df2"
      ],
      "metadata": {
        "id": "L2t3uwNRY-pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 아예 `drop_duplicates()`를 사용할 수도 있다."
      ],
      "metadata": {
        "id": "6OvgzApFISoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.drop_duplicates(['id', 'fname', 'lname'], keep='last')      #'last' 말고 'first' 또는 False 라면?\n",
        "df2"
      ],
      "metadata": {
        "id": "jI14zkj2bAV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}